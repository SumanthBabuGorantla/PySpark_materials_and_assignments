{"cells":[{"cell_type":"code","source":["#import pyspark.sql.functions as f\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/first_test.csv\")  \ndf.show()\n#df.printSchema()\n#df.select(\"name\",\"id\").show()\n#df.filter(df.id>3).show()\n#df.select(df.name, df.zip + 1).show()\n#df.groupBy(\"zip\").sum(\"id\").show()\n\n#df.agg({\"id\": \"max\"}).show() # 6\n#df.agg({\"id\": \"min\"}).show() # 1\n#df.agg({\"id\": \"count\"}).show() # 5\n#df.agg({\"id\": \"avg\"}).show() # 3.2\n#print(df.columns)\n\n\n#df.filter((df.id == 1) & (df.zip == 560097)).show()\n\n#df.filter((df.id == 1) | (df.zip == 560098)).show()\n#new_df = df.where(df.id>2).select(\"id\",\"zip\")\n#new_df.show()\n#print(df.collect())\n\n'''for i in df.collect():\n  print(i[\"id\"],i[\"area\"])'''\n\ndf.createOrReplaceTempView(\"test\") # dataframe u have regeistered as test table\n#spark.sql(\" select * from test \").show()\nspark.sql(\" select * from test where id> 3\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c876904-7ecb-44ef-9ac1-fd53b8aa13ee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+----+\n| id|   area|  name|   zip| _c4|\n+---+-------+------+------+----+\n|  1|BLOCK A|   RAM|560097|null|\n|  2|BLOCK B|   RAJ|560091|null|\n|  3|BLOCK C| ROHAN|560092|null|\n|  4|BLOCK D|RAMESH|560092|null|\n|  5|BLOCK E|  RAMU|560098|null|\n|  6|BLOCK S|   Dam|450000|null|\n+---+-------+------+------+----+\n\n+---+-------+------+------+----+\n| id|   area|  name|   zip| _c4|\n+---+-------+------+------+----+\n|  4|BLOCK D|RAMESH|560092|null|\n|  5|BLOCK E|  RAMU|560098|null|\n|  6|BLOCK S|   Dam|450000|null|\n+---+-------+------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nfile_location1 = \"/FileStore/tables/first_test.csv\"\nfile_location2 = \"/FileStore/tables/first_test.csv\"\nfile_location3 = \"/FileStore/tables/second.csv\"\ndf1 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_location1)\ndf2 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_location2)\n#df1.show()\n#df2.show()\n#df2 = df2.drop(\"id\",\"area\")\n#df2 = df2.drop(\"id\",\"area\")\n#df2.show()\n#except_res = df1.substract(df2)\n#except_res.show()\n#intersection_res = df1.intersect(df2)\n#intersection_res.show()\n#union_res = df1.union(df2)\n#union_res.show()\n#df1.orderBy(f.asc(\"zip\")).show() # select * from test order by zip asc\n#df1.orderBy(f.desc(\"zip\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e25d053-d192-4c7a-af5b-cb11fe0aecd1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd1 = sc.parallelize([(\"Rafferty\",31),(\"Jones\",33),(\"Heisenberg\", 33),(\"Robinson\", 34),(\"Smith\", 34),(\"Williams\", 39)])\nrdd2 = sc.parallelize([(31, \"Sales\"),(33, \"Engineering\"),(34, \"Clerical\"),(35, \"Marketing\")])\nemployees = rdd1.toDF([\"LastName\",\"DepartmentID\"])\ndepartments = rdd2.toDF([\"DepartmentID\",\"DepartmentName\"])\nemployees.show()\ndepartments.show()\n#df1 =  employees.join(departments, \"DepartmentID\")  # select * from test where test.id = test1.id and test.name = test1.name\n#df1.show()\n#inner_join.show()\n#inner_join_with_Multiple = employees.join(departments, [\"DepartmentID\"])\n#inner_join_with_Multiple.show()\n#left = employees.join(departments, \"DepartmentID\", \"left\")\n#left.show()\n#left_outer = employees.join(departments, \"DepartmentID\", \"left_outer\")\n#left_outer.show()\n#left_semi = employees.join(departments, \"DepartmentID\", \"left_semi\")\n#left_semi.show()\n#right = employees.join(departments, \"DepartmentID\", \"right\")\n#right.show()\n#right_outer = employees.join(departments, \"DepartmentID\", \"right_outer\")\n#right_outer.show()\n#fullouter = employees.join(departments, \"DepartmentID\", \"fullouter\")\n#fullouter.show()\nleft_anti = employees.join(departments, \"DepartmentID\", \"left_anti\")\nleft_anti.show()\n#cartesian_join =  employees.join(departments) \n#cartesian_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d29e68c2-1794-4063-96ac-b2ed24943a42","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------------+\n|  LastName|DepartmentID|\n+----------+------------+\n|  Rafferty|          31|\n|     Jones|          33|\n|Heisenberg|          33|\n|  Robinson|          34|\n|     Smith|          34|\n|  Williams|          39|\n+----------+------------+\n\n+------------+--------------+\n|DepartmentID|DepartmentName|\n+------------+--------------+\n|          31|         Sales|\n|          33|   Engineering|\n|          34|      Clerical|\n|          35|     Marketing|\n+------------+--------------+\n\n+------------+--------+\n|DepartmentID|LastName|\n+------------+--------+\n|          39|Williams|\n+------------+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nrdd1 = sc.parallelize([(\"Rafferty\",31),(\"Jones\",33),(\"Heisenberg\", 33),(\"Robinson\", 34),(\"Smith\", 34),(\"Williams\", 39)])\n#rdd1.collect()\nemployees = rdd1.toDF([\"LastName\",\"DepartmentID\"]) # converstion of rdd to dataframe \n#employees.show()\n#employees = employees.withColumn(\"salary\", f.lit(7500))\n#employees.show()\nemployees = employees.withColumnRenamed(\"LastName\",\"LastName_name\")\nemployees.show()\n#employees."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"943c3ba8-7981-4820-9ebb-1d8c2dc9b13a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+------------+\n|LastName_name|DepartmentID|\n+-------------+------------+\n|     Rafferty|          31|\n|        Jones|          33|\n|   Heisenberg|          33|\n|     Robinson|          34|\n|        Smith|          34|\n|     Williams|          39|\n+-------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\nfile_location1 = \"/FileStore/tables/first_test.csv\"\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_location1)\n#df.show()\n#df.printSchema()\ndf = df.withColumn(\"id\", df[\"id\"].cast(IntegerType()))\ndf = df.withColumn(\"zip\", df[\"zip\"].cast(IntegerType()))\ndf.show()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6153286e-238f-44af-94ab-d91ce88e6f6b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   area|  name|   zip|\n+---+-------+------+------+\n|  1|BLOCK A|   RAM|560097|\n|  2|BLOCK B|   RAJ|560091|\n|  3|BLOCK C| ROHAN|560092|\n|  4|BLOCK D|RAMESH|560092|\n|  6|BLOCK E|  RAMU|560098|\n+---+-------+------+------+\n\nroot\n |-- id: integer (nullable = true)\n |-- area: string (nullable = true)\n |-- name: string (nullable = true)\n |-- zip: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["file_location1 = \"/FileStore/tables/Test_DataMissing.csv\"\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_location1)\ndf.show()\ndf.printSchema()\n#df.na.drop().show()  # it will remove all record which is having a single null also\n#df.na.fill(100).show()  # it will fill the value for those column which is having Number Type(Int or Float)\n#df.na.fill(\"Missing_Number\").show() # it will fill the value for those column which is having String Type#\n#df.na.fill(\"Missing_Number\",[\"Id\"]).show() # it will fill the multiple Column"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5fc5d051-2563-491d-b75f-ab2b9df2ec2a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John| null|\n|emp2| null| null|\n|emp3| null|345.0|\n| em4|Cindy|456.0|\n+----+-----+-----+\n\nroot\n |-- Id: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Sales: double (nullable = true)\n\n+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John| null|\n|emp2| null| null|\n|emp3| null|345.0|\n| em4|Cindy|456.0|\n+----+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import functions as f\ndf = sc.parallelize([(4, \"blah\", 2),(2, \"\", 3),(56, \"foo\", 3),(100, \"bar\", 5)]).toDF([\"A\", \"B\", \"C\"])\ndf.show()\n#df.createOrReplaceTempView(\"test\")\n\n#res = spark.sql(\"select * from test\")\n#res.show()\n''\nnewDf = df.withColumn(\"D\", f.when(df.B == \"\", 0).otherwise(1))\nnewDf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f8861f4-ec3d-49ef-beac-280f484fcf3c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+---+\n|  A|   B|  C|\n+---+----+---+\n|  4|blah|  2|\n|  2|    |  3|\n| 56| foo|  3|\n|100| bar|  5|\n+---+----+---+\n\n+---+----+---+---+\n|  A|   B|  C|  D|\n+---+----+---+---+\n|  4|blah|  2|  1|\n|  2|    |  3|  0|\n| 56| foo|  3|  1|\n|100| bar|  5|  1|\n+---+----+---+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import functions as f\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/Test_Date2.csv\")\ndf.show()\ndf.printSchema()\nmodifiedDF = df.withColumn(\"Date\", f.to_date(df.Date, \"MM/dd/yyyy\"))\nmodifiedDF = modifiedDF.withColumn(\"birth_year\",f.year(modifiedDF.Date))\nmodifiedDF = modifiedDF.withColumn(\"birth_month\",f.month(modifiedDF.Date))\nmodifiedDF = modifiedDF.withColumn(\"birth_date\",f.dayofmonth(modifiedDF.Date))\nmodifiedDF = modifiedDF.withColumn(\"fivteen_days_later\",f.date_add(modifiedDF.Date, 15))\nmodifiedDF = modifiedDF.withColumn(\"twentty_days_Before\",f.date_add(modifiedDF.Date, -20))\n#modifiedDF.show()\nmodifiedDF.show()\n#modifiedDF.printSchema()\n#modifiedDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6a370f3e-4897-4bb1-8b9c-880e3bd146dd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---+------+---+\n|      Date| id|Volume|Age|\n+----------+---+------+---+\n|08/26/2016|100|  90.7| 26|\n|08/27/2016|101|  90.7| 26|\n|08/26/2016|102|  90.7| 26|\n|06/14/2016|103|  90.7| 26|\n|06/19/2016|102|  90.7| 26|\n|06/25/2016|104|  90.5| 27|\n+----------+---+------+---+\n\nroot\n |-- Date: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- Volume: double (nullable = true)\n |-- Age: integer (nullable = true)\n\n+----------+---+------+---+----------+-----------+----------+------------------+-------------------+\n|      Date| id|Volume|Age|birth_year|birth_month|birth_date|fivteen_days_later|twentty_days_Before|\n+----------+---+------+---+----------+-----------+----------+------------------+-------------------+\n|2016-08-26|100|  90.7| 26|      2016|          8|        26|        2016-09-10|         2016-08-06|\n|2016-08-27|101|  90.7| 26|      2016|          8|        27|        2016-09-11|         2016-08-07|\n|2016-08-26|102|  90.7| 26|      2016|          8|        26|        2016-09-10|         2016-08-06|\n|2016-06-14|103|  90.7| 26|      2016|          6|        14|        2016-06-29|         2016-05-25|\n|2016-06-19|102|  90.7| 26|      2016|          6|        19|        2016-07-04|         2016-05-30|\n|2016-06-25|104|  90.5| 27|      2016|          6|        25|        2016-07-10|         2016-06-05|\n+----------+---+------+---+----------+-----------+----------+------------------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import functions as f\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/Test_TimeStamp.csv\")\n#df.show()\n#df.printSchema()\ndf = df.withColumn(\"minute\",f.minute(df.DateTime))\ndf = df.withColumn(\"second\",f.second(df.DateTime))\ndf = df.withColumn(\"hour\",f.hour(df.DateTime))\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e505baf-9fda-4f76-92f1-78be9d8832db","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------+-----+------+----+------+------+----+\n|           DateTime|   id|Volume| Age|minute|second|hour|\n+-------------------+-----+------+----+------+------+----+\n|2017-08-02 03:04:00|100.0|  90.7|  26|     4|     0|   3|\n|1999-07-05 01:45:20| 90.7|  26.0|null|    45|    20|   1|\n|1998-08-06 01:35:20|102.0|  90.7|  26|    35|    20|   1|\n|1997-09-07 01:25:19|103.0|  90.7|  26|    25|    19|   1|\n|1996-03-03 01:15:18|102.0|  90.7|  26|    15|    18|   1|\n|1995-09-05 01:05:17|104.0|  90.5|  27|     5|    17|   1|\n+-------------------+-----+------+----+------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# if i am passing id column in the udf , it should return greater than 3 if the value is greater than 3 else it should return as lesser than 3 as new Column\nfrom pyspark.sql.functions import udf,col\nfile_location1 = \"/FileStore/tables/first_test.csv\"\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(file_location1)\ndf.show()\ndef CheckValue(id):\n  if(id>3):\n    return \"Greater than 3\"\n  elif(id<3):\n    return \"Lesser than 3\"\n  else:\n    return \"equals to 3\"\na = udf(CheckValue)\ndf_res = df.withColumn('res',a(col('id')))\ndf_res.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a85cc71c-4543-4b37-9480-a2394265d382","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   area|  name|   zip|\n+---+-------+------+------+\n|  1|BLOCK A|   RAM|560097|\n|  2|BLOCK B|   RAJ|560091|\n|  3|BLOCK C| ROHAN|560092|\n|  4|BLOCK D|RAMESH|560092|\n|  6|BLOCK E|  RAMU|560098|\n+---+-------+------+------+\n\n+---+-------+------+------+--------------+\n| id|   area|  name|   zip|           res|\n+---+-------+------+------+--------------+\n|  1|BLOCK A|   RAM|560097| Lesser than 3|\n|  2|BLOCK B|   RAJ|560091| Lesser than 3|\n|  3|BLOCK C| ROHAN|560092|   equals to 3|\n|  4|BLOCK D|RAMESH|560092|Greater than 3|\n|  6|BLOCK E|  RAMU|560098|Greater than 3|\n+---+-------+------+------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import re  # Regex Library\ntxt = \"1.1..1.1\"\ncompiled_pattern = re.compile('^[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}$')\ndef CheckValidIP(IP):\n  res_Object = compiled_pattern.match(IP)\n  #print(res_Object)\n  if(res_Object):\n    return IP\n  else:\n    return \"Invalid IP \" + IP  \nres = CheckValidIP(txt)  \nprint(res)  \n\n#  /FileStore/tables/abhisek_task.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39b00a24-4b7e-4803-ac40-2081db2d1662","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# aakashkumar667@gmail.com\nfrom pyspark.sql.functions import udf,col\nfile_location1 = \"/FileStore/tables/first_test.csv\"\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(file_location1)\n#df.show()\n#df.printSchema()\n\ndict={}\nfor i in df.collect():\n    #print(i['id'])\n    #print(i['name'])\n    dict[i['id']]=i['name']\nprint(dict)\n\n\n\ndef CheckValue(id):\n  #print(dict.get(id))\n  return str(dict.get(id)) + \"_new\"\n\na = udf(CheckValue) # here we are registering that function as udf , function will do for only primitive and data strutre data types , but if u have to pass the columns you must have to register the function as the udf\ndf_res = df.withColumn('res',a(col('id')))\ndf_res.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9e59e79-8b22-47d6-9b33-5fe62d53b98a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["{1: 'RAM', 2: 'RAJ', 3: 'ROHAN', 4: 'RAMESH', 6: 'RAMU'}\n+---+-------+------+------+----------+\n| id|   area|  name|   zip|       res|\n+---+-------+------+------+----------+\n|  1|BLOCK A|   RAM|560097|   RAM_new|\n|  2|BLOCK B|   RAJ|560091|   RAJ_new|\n|  3|BLOCK C| ROHAN|560092| ROHAN_new|\n|  4|BLOCK D|RAMESH|560092|RAMESH_new|\n|  6|BLOCK E|  RAMU|560098|  RAMU_new|\n+---+-------+------+------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["dfs = spark.read.option(\"inferSchema\",\"true\").option(\"multiline\", \"true\").json(\"/FileStore/tables/first.json\")\ndfs.show()\ndfs.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f966f3c-9c7f-4a2a-9d21-cf550bd9cef4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+-------+\n|age|  id|   name|\n+---+----+-------+\n| 25|1201| satish|\n| 28|1202|krishna|\n| 39|1203|  amith|\n| 23|1204|  javed|\n| 23|1205| prudvi|\n+---+----+-------+\n\nroot\n |-- age: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import functions as f\ndf = spark.read.json(\"/FileStore/tables/second.json\")\ndf.show(truncate=False)\n#df.printSchema()\n#dfDates = df.select(f.explode(df.dates))\n#dfDates.show()\ndfContent = df.select(f.explode(df.content))\ndfContent.show()\ndfFooBar = dfContent.select(\"col.id\", \"col.value\").show()\n'''dfContent.printSchema()\ndfContent1 = df.select(f.explode(df.content)).toDF(\"content\")\ndfContent1.show()\ndfFooBar = dfContent1.select(\"content.id\", \"content.value\").show()'''"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"efbccc2f-5c16-462d-96f7-0deacbdeeffc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------------------------------------------------------+------------------------+-----------+------+-----------+\n|content                                                          |dates                   |reason     |status|user       |\n+-----------------------------------------------------------------+------------------------+-----------+------+-----------+\n|[{123, val1}, {456, val2}, {789, val3}, {124, val4}, {126, val5}]|[2016-01-29, 2016-01-28]|some reason|OK    |gT35Hhhre9m|\n+-----------------------------------------------------------------+------------------------+-----------+------+-----------+\n\n+-----------+\n|        col|\n+-----------+\n|{123, val1}|\n|{456, val2}|\n|{789, val3}|\n|{124, val4}|\n|{126, val5}|\n+-----------+\n\n+---+-----+\n| id|value|\n+---+-----+\n|123| val1|\n|456| val2|\n|789| val3|\n|124| val4|\n|126| val5|\n+---+-----+\n\nOut[7]: 'dfContent.printSchema()\\ndfContent1 = df.select(f.explode(df.content)).toDF(\"content\")\\ndfContent1.show()\\ndfFooBar = dfContent1.select(\"content.id\", \"content.value\").show()'"]}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/first_test.csv\")  \ndf.show()\ndf.write.parquet(\"/FileStore/tables/first_test.parquet\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f46b4b85-0c00-4b8f-8cc5-bcf2f062a7f3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["first_number = 10\nsecond_number = 20\nres_add = first_number + second_number \nprint(res_add)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5428367-8ce0-47d1-b6c8-10641ad338bf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nrdd1 = sc.parallelize([(\"06/12/2019\",31)])\nemployees = rdd1.toDF([\"LastName\",\"DepartmentID\"])\nemployees.show()\nget_date = employees.first().DepartmentID\nemployees.show()\n'''get_Query = \"select * from demo where x = \"+ get_date \nprint(get_Query)'''\nprint(get_date)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b349602-09bf-4abe-bb64-435110839c75","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nrdd1 = sc.parallelize([(\"CX_CXHI_WI\",\"AMP_CHURN\"),(\"CX_CXHI_WI\",\"AMP_CHURN2\"),(\"CX_CXHI_WI\",\"AMP_CHURN4\")])\nsch = rdd1.toDF([\"SCHEMA_NAME\",\"TBL_NAME\"])\nsch.show()\nstr1 = \"\"\nfor i in sch.collect():\n  str1 = str1 + \"select * from \" + i['SCHEMA_NAME'] + \".\" + i['TBL_NAME'] + \",\"\nl = str1.split(\",\")\nfor i in range(0,len(l)-1):\n  print(l[i])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ade2086c-8f49-4468-80ce-9cfc76818602","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["l1 = []\ndf = sc.emptyRDD().toDF(\"\")\nrdd1 = sc.parallelize([(\"CX_CXHI_WI\",\"AMP_CHURN\"),(\"CX_CXHI_WI\",\"AMP_CHURN2\"),(\"CX_CXHI_WI\",\"AMP_CHURN4\")])\nsch = rdd1.toDF([\"SCHEMA_NAME\",\"TBL_NAME\"])\nfor i in range(0,sch.count()):\n  l1.append(sc.emptyRDD().toDF(\"\"))\nprint(l1)  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36441bbd-91d9-4f41-a5ff-ba154bd36941","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["str = [\"/FileStore/tables/first_test.csv\",\"/FileStore/tables/first_test.csv\"]\nrdd1 = sc.parallelize([(\" \",\" \",\" \",\" \")])\ndf = rdd1.toDF([\"id\",\"area\",\"name\",\"zip\"])\nfor i in str:\n  df1 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(i)\n  df = df.union(df1)\ndf.show()  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8b0098a-5648-4e08-8728-12eac534fe63","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["x = [1,2,2,3,4,5,5,6,9]\ny = []\n#print(x)\nfor i in x:\n  if i not in y:\n    y.append(i)\nprint(y)    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9818e9bf-f2f9-484c-99b0-0d952517de66","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/pokemon.csv\")\ndf = df.withColumnRenamed(\"Type 2\",\"Type_2\")\n\n\n\n\ndef CheckValue(Total):\n  if(Total>200):\n    return 0\n  else:\n    return Total\na = F.udf(CheckValue)\ndf.withColumn('res',a(F.col('Total'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89256ac5-bed9-4fce-9e4f-7ee03c3ca188","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import datetime\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.types import IntegerType,DoubleType\ndf1 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/ratings.csv\")  \n#df1.show()\ndf1.printSchema()\n#df2 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/tags.csv\")  \n#df2.show()\ndf3 = df1.withColumn(\"timestamp\",df1[\"timestamp\"].cast(DoubleType()))\ndf3.printSchema()\ndf4 = df1.withColumn(\"newtimestamp\",datetime.datetime.fromtimestamp(\"timestamp\"))\ndf4.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c753a40-8863-42cf-9806-b3fdb1a78733","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["arr = [5, 2, 8, 7, 1];     \ntemp = 0;    \n     \n#Displaying elements of original array    \nprint(\"Elements of original array: \");    \nfor i in range(0, len(arr)):    \n    print(arr[i], end=\" \");    \n     \n#Sort the array in ascending order    \nfor i in range(0, len(arr)):    \n    for j in range(i+1, len(arr)):    \n        if(arr[i] > arr[j]):    \n            temp = arr[i];    \n            arr[i] = arr[j];    \n            arr[j] = temp;  \nprint(arr)            "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b032b48c-33c5-4d92-a1f4-8729240cd9a1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/first_test.csv\")  \ndf.show()\ndf.persist(pyspark.StorageLevel.DISK_ONLY)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83d85c86-6f83-49e9-aadf-467a3b1c939a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark import SparkContext\nimport pyspark\nsc = SparkContext (\n   \"local\", \n   \"storagelevel app\"\n)\nrdd1 = sc.parallelize([1,2])\nrdd1.persist( pyspark.StorageLevel.MEMORY_AND_DISK_2 )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ffe30f34-56ec-42e5-8cbb-a7ed87e69f7a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import functions as F\nrdd1 = sc.parallelize([(\"000006663769\",\"0001152200\"),(\"000006663769\",\"0001152200\")])\ndf = rdd1.toDF([\"process_order\",\"root_batch\"])\n#df.show()\ndf = df.withColumn('process_order', F.regexp_replace('process_order', r'0', ''))\ndf = df.withColumn('root_batch', F.regexp_replace('root_batch', r'0', ''))\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"95fffe6a-13e0-4e2d-8408-93b32dd70152","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"process_order","nullable":true,"type":"string"},{"metadata":{},"name":"root_batch","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-------------+----------+\n|process_order|root_batch|\n+-------------+----------+\n|      6663769|     11522|\n|      6663769|     11522|\n+-------------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-------------+----------+\n","process_order|root_batch|\n","+-------------+----------+\n","      6663769|     11522|\n","      6663769|     11522|\n","+-------------+----------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def data(num):\n  if(num<3):\n    return \"lesser than 3\"\n  elif(num>3):\n    return \"Greater than 3\"\n  else:\n    return \"equal\"\nres = data(5)  \nprint(res)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c4be2e8-7347-41e2-a028-a681ad6611a5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Greater than 3\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Greater than 3\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# if i am passing id column in the udf , it should return greater than 3 if the value is greater than 3 else it should return as lesser than 3 as new Column\nfrom pyspark.sql.functions import udf,col\nfile_location1 = \"/FileStore/tables/first_test.csv\"\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(file_location1)\ndf.show()\ndef data(num):\n  if(num<3):\n    return \"lesser than 3\"\n  elif(num>3):\n    return \"Greater than 3\"\n  else:\n    return \"equal\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f9cc6c4-ca49-4014-b4e7-4243100279fb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"integer"},{"metadata":{},"name":"area","nullable":true,"type":"string"},{"metadata":{},"name":"name","nullable":true,"type":"string"},{"metadata":{},"name":"zip","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+---+-------+------+------+\n| id|   area|  name|   zip|\n+---+-------+------+------+\n|  1|BLOCK A|   RAM|560097|\n|  2|BLOCK B|   RAJ|560091|\n|  3|BLOCK C| ROHAN|560092|\n|  4|BLOCK D|RAMESH|560092|\n|  6|BLOCK E|  RAMU|560098|\n+---+-------+------+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+---+-------+------+------+\n"," id|   area|  name|   zip|\n","+---+-------+------+------+\n","  1|BLOCK A|   RAM|560097|\n","  2|BLOCK B|   RAJ|560091|\n","  3|BLOCK C| ROHAN|560092|\n","  4|BLOCK D|RAMESH|560092|\n","  6|BLOCK E|  RAMU|560098|\n","+---+-------+------+------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import functions as f\ndf = sc.parallelize([(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)]).toDF([\"employee_name\", \"department\", \"salary\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"259e051c-c6df-4c38-9a48-125d205e67fb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["  '''\n  row_number Window Function\nrow_number() window function is used to give the sequential row number starting from 1 to the result of each window partition.'''\n  \nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nwindowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\nres = df.withColumn(\"row_number\",row_number().over(windowSpec))\nres.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a05b6175-89f8-43c0-bf11-a357769e6199","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         2|\n|       Robert|     Sales|  4100|         3|\n|         Saif|     Sales|  4100|         4|\n|      Michael|     Sales|  4600|         5|\n+-------------+----------+------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["'''\nrank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n'''\nfrom pyspark.sql.functions import rank\nres1 = df.withColumn(\"rank\",rank().over(windowSpec))\nres1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d59c47d1-e4ea-4abd-8d3b-e5899e689f38","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   1|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   3|\n|      Michael|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["'''\ndense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties.'''\nfrom pyspark.sql.functions import dense_rank\nres3=df.withColumn(\"dense_rank\",dense_rank().over(windowSpec))\nres3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c71a11f-14a1-4af5-906e-2fa9402dc8db","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["'''This is the same as the LAG function in SQL.'''\nfrom pyspark.sql.functions import lag    \nfrom pyspark.sql.functions import lag    \ndf.withColumn(\"lag\",lag(\"salary\",1).over(windowSpec)) \\\n      .show()                "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e59048b6-75e5-43f5-b09b-7282d6d379c5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|null|\n|        Scott|   Finance|  3300|3000|\n|          Jen|   Finance|  3900|3300|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|2000|\n|        James|     Sales|  3000|null|\n|        James|     Sales|  3000|3000|\n|       Robert|     Sales|  4100|3000|\n|         Saif|     Sales|  4100|4100|\n|      Michael|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["'''\nThis is the same as the LEAD function in SQL\n'''\n\nfrom pyspark.sql.functions import lead    \ndf.withColumn(\"lead\",lead(\"salary\",1).over(windowSpec)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f848fb55-7b8a-441e-8dc7-e1609a2717c3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|3300|\n|        Scott|   Finance|  3300|3900|\n|          Jen|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|3000|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|3000|\n|        James|     Sales|  3000|4100|\n|       Robert|     Sales|  4100|4100|\n|         Saif|     Sales|  4100|4600|\n|      Michael|     Sales|  4600|null|\n+-------------+----------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["'''\nIn this section, I will explain how to calculate sum, min, max for each department using PySpark SQL Aggregate window functions and WindowSpec. When working with Aggregate functions, we don’t need to use order by clause.\n'''\nwindowSpecAgg  = Window.partitionBy(\"department\")\nfrom pyspark.sql.functions import col,avg,sum,min,max,row_number \ndf.withColumn(\"row\",row_number().over(windowSpec)) \\\n  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a7926b23-25c8-495b-b6d8-1d7d43335633","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|     Sales|3760.0|18800|3000|4600|\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n+----------+------+-----+----+----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+----------+------+-----+----+----+\n","department|   avg|  sum| min| max|\n","+----------+------+-----+----+----+\n","     Sales|3760.0|18800|3000|4600|\n","   Finance|3400.0|10200|3000|3900|\n"," Marketing|2500.0| 5000|2000|3000|\n","+----------+------+-----+----+----+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\nx = [(\"2001\",\"id1\"),(\"2002\",\"id1\"),(\"2002\",\"id1\"),(\"2001\",\"id1\"),(\"2001\",\"id2\"),(\"2001\",\"id2\"),(\"2002\",\"id2\")]\ny = spark.createDataFrame(x,[\"year\",\"id\"])\n\ngr = y.groupBy(\"year\").agg(countDistinct(\"id\"))\ngr.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9eab22d-7745-4afb-be02-f9d4641d2f82","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"y","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"year","nullable":true,"type":"string"},{"metadata":{},"name":"id","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"gr","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"year","nullable":true,"type":"string"},{"metadata":{},"name":"count(DISTINCT id)","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+----+------------------+\n|year|count(DISTINCT id)|\n+----+------------------+\n|2002|                 2|\n|2001|                 2|\n+----+------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+----+------------------+\n","year|count(DISTINCT id)|\n","+----+------------------+\n","2002|                 2|\n","2001|                 2|\n","+----+------------------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\n\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").option(\"delimiter\",\"|\").load(\"/FileStore/tables/msidn.txt\")\n#df.show()\ndata1 = df.groupBy(\"APP_ID\",\"APP_GROUP\",\"APP_NAME\").count()\n#data1.show()\ndata1 = data1.withColumnRenamed(\"count\",\"UNIQUE_USER_COUNT\")\n\ndata2 = df.groupBy(\"APP_GROUP\").agg(f.countDistinct(\"MSISDN\"))\ndata2 = data2.withColumnRenamed(\"count(DISTINCT MSISDN)\",\"UNIQU_GROUP_COUNT\")\n#data2.show()\n\nget_top5 = data2.orderBy(f.desc(\"UNIQU_GROUP_COUNT\")).limit(5).select(\"APP_GROUP\").rdd.flatMap(lambda x: x).collect()\n#get_top5\nfiltered_data_selected_group = data1.filter(f.col(\"APP_GROUP\").isin(get_top5))\n\nres_data = filtered_data_selected_group.join(data2,\"APP_GROUP\")\nres_data.show()\n\n#windowSpec  = Window.partitionBy(\"APP_GROUP\").orderBy(desc(\"UNIQUE_USER_COUNT\"))\n\nwindowSpec  = Window.partitionBy(\"APP_GROUP\").orderBy(f.desc(\"UNIQUE_USER_COUNT\"))\ngrouped_data = res_data.withColumn(\"row_number\",row_number().over(windowSpec))\n#grouped_data.show()\n# #res = df.withColumn(\"row_number\",row_number().over(windowSpec))\n\ninter_mediate_data = grouped_data.withColumn(\"row_number\", row_number().over(windowSpec)).where(f.col(\"row_number\")<=5)\nfinal_result = inter_mediate_data.drop(\"row_number\")\nfinal_result.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0482b92e-1cd8-444a-8b23-3bbdeeb85711","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"MSISDN","nullable":true,"type":"integer"},{"metadata":{},"name":"APP_ID","nullable":true,"type":"integer"},{"metadata":{},"name":"APP_GROUP","nullable":true,"type":"string"},{"metadata":{},"name":"APP_NAME","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"data1","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"APP_ID","nullable":true,"type":"integer"},{"metadata":{},"name":"APP_GROUP","nullable":true,"type":"string"},{"metadata":{},"name":"APP_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"UNIQUE_USER_COUNT","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null},{"name":"data2","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"APP_GROUP","nullable":true,"type":"string"},{"metadata":{},"name":"UNIQU_GROUP_COUNT","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null},{"name":"filtered_data_selected_group","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"APP_ID","nullable":true,"type":"integer"},{"metadata":{},"name":"APP_GROUP","nullable":true,"type":"string"},{"metadata":{},"name":"APP_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"UNIQUE_USER_COUNT","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null},{"name":"res_data","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"APP_GROUP","nullable":true,"type":"string"},{"metadata":{},"name":"APP_ID","nullable":true,"type":"integer"},{"metadata":{},"name":"APP_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"UNIQUE_USER_COUNT","nullable":false,"type":"long"},{"metadata":{},"name":"UNIQU_GROUP_COUNT","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null},{"name":"grouped_data","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"APP_GROUP","nullable":true,"type":"string"},{"metadata":{},"name":"APP_ID","nullable":true,"type":"integer"},{"metadata":{},"name":"APP_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"UNIQUE_USER_COUNT","nullable":false,"type":"long"},{"metadata":{},"name":"UNIQU_GROUP_COUNT","nullable":false,"type":"long"},{"metadata":{},"name":"row_number","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null},{"name":"inter_mediate_data","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"APP_GROUP","nullable":true,"type":"string"},{"metadata":{},"name":"APP_ID","nullable":true,"type":"integer"},{"metadata":{},"name":"APP_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"UNIQUE_USER_COUNT","nullable":false,"type":"long"},{"metadata":{},"name":"UNIQU_GROUP_COUNT","nullable":false,"type":"long"},{"metadata":{},"name":"row_number","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null},{"name":"final_result","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"APP_GROUP","nullable":true,"type":"string"},{"metadata":{},"name":"APP_ID","nullable":true,"type":"integer"},{"metadata":{},"name":"APP_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"UNIQUE_USER_COUNT","nullable":false,"type":"long"},{"metadata":{},"name":"UNIQU_GROUP_COUNT","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-----------+------+---------+-----------------+-----------------+\n|  APP_GROUP|APP_ID| APP_NAME|UNIQUE_USER_COUNT|UNIQU_GROUP_COUNT|\n+-----------+------+---------+-----------------+-----------------+\n|Webbrowsing|    59|  Twitter|                8|               10|\n|Webbrowsing|    60|Instagram|                2|               10|\n|Webbrowsing|    58|    Fauji|               11|               10|\n|       Game|   581|    Fauji|               11|               16|\n|Webbrowsing|    56| Whatsapp|                4|               10|\n|       Game|   591|    8poll|                8|               16|\n|Webbrowsing|    57|   tinder|                6|               10|\n|       Game|   601|  fotball|                2|               16|\n|       Game|   551|    Carem|                4|               16|\n|       Game|   561|     Ludo|                7|               16|\n|Webbrowsing|    55| Facebook|                3|               10|\n|       Game|   571|  Cricket|                6|               16|\n+-----------+------+---------+-----------------+-----------------+\n\n+-----------+------+--------+-----------------+-----------------+\n|  APP_GROUP|APP_ID|APP_NAME|UNIQUE_USER_COUNT|UNIQU_GROUP_COUNT|\n+-----------+------+--------+-----------------+-----------------+\n|       Game|   581|   Fauji|               11|               16|\n|       Game|   591|   8poll|                8|               16|\n|       Game|   561|    Ludo|                7|               16|\n|       Game|   571| Cricket|                6|               16|\n|       Game|   551|   Carem|                4|               16|\n|Webbrowsing|    58|   Fauji|               11|               10|\n|Webbrowsing|    59| Twitter|                8|               10|\n|Webbrowsing|    57|  tinder|                6|               10|\n|Webbrowsing|    56|Whatsapp|                4|               10|\n|Webbrowsing|    55|Facebook|                3|               10|\n+-----------+------+--------+-----------------+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-----------+------+---------+-----------------+-----------------+\n","  APP_GROUP|APP_ID| APP_NAME|UNIQUE_USER_COUNT|UNIQU_GROUP_COUNT|\n","+-----------+------+---------+-----------------+-----------------+\n","Webbrowsing|    59|  Twitter|                8|               10|\n","Webbrowsing|    60|Instagram|                2|               10|\n","Webbrowsing|    58|    Fauji|               11|               10|\n","       Game|   581|    Fauji|               11|               16|\n","Webbrowsing|    56| Whatsapp|                4|               10|\n","       Game|   591|    8poll|                8|               16|\n","Webbrowsing|    57|   tinder|                6|               10|\n","       Game|   601|  fotball|                2|               16|\n","       Game|   551|    Carem|                4|               16|\n","       Game|   561|     Ludo|                7|               16|\n","Webbrowsing|    55| Facebook|                3|               10|\n","       Game|   571|  Cricket|                6|               16|\n","+-----------+------+---------+-----------------+-----------------+\n","\n","+-----------+------+--------+-----------------+-----------------+\n","  APP_GROUP|APP_ID|APP_NAME|UNIQUE_USER_COUNT|UNIQU_GROUP_COUNT|\n","+-----------+------+--------+-----------------+-----------------+\n","       Game|   581|   Fauji|               11|               16|\n","       Game|   591|   8poll|                8|               16|\n","       Game|   561|    Ludo|                7|               16|\n","       Game|   571| Cricket|                6|               16|\n","       Game|   551|   Carem|                4|               16|\n","Webbrowsing|    58|   Fauji|               11|               10|\n","Webbrowsing|    59| Twitter|                8|               10|\n","Webbrowsing|    57|  tinder|                6|               10|\n","Webbrowsing|    56|Whatsapp|                4|               10|\n","Webbrowsing|    55|Facebook|                3|               10|\n","+-----------+------+--------+-----------------+-----------------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/zomato.csv\")  \ndf = df.withColumnRenamed(\"Price range\",\"Price_range\")\ndf.createOrReplaceTempView(\"test\")\nspark.sql(\"select City,sum(Price_range) from test group by City\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"84f8690e-881f-486b-b39e-2245f5541ed4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Restaurant ID","nullable":true,"type":"string"},{"metadata":{},"name":"Restaurant Name","nullable":true,"type":"string"},{"metadata":{},"name":"Country Code","nullable":true,"type":"string"},{"metadata":{},"name":"City","nullable":true,"type":"string"},{"metadata":{},"name":"Address","nullable":true,"type":"string"},{"metadata":{},"name":"Locality","nullable":true,"type":"string"},{"metadata":{},"name":"Locality Verbose","nullable":true,"type":"string"},{"metadata":{},"name":"Longitude","nullable":true,"type":"string"},{"metadata":{},"name":"Latitude","nullable":true,"type":"string"},{"metadata":{},"name":"Cuisines","nullable":true,"type":"string"},{"metadata":{},"name":"Average Cost for two","nullable":true,"type":"string"},{"metadata":{},"name":"Currency","nullable":true,"type":"string"},{"metadata":{},"name":"Has Table booking","nullable":true,"type":"string"},{"metadata":{},"name":"Has Online delivery","nullable":true,"type":"string"},{"metadata":{},"name":"Is delivering now","nullable":true,"type":"string"},{"metadata":{},"name":"Switch to order menu","nullable":true,"type":"string"},{"metadata":{},"name":"Price_range","nullable":true,"type":"string"},{"metadata":{},"name":"Aggregate rating","nullable":true,"type":"string"},{"metadata":{},"name":"Rating color","nullable":true,"type":"string"},{"metadata":{},"name":"Rating text","nullable":true,"type":"string"},{"metadata":{},"name":"Votes","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+--------------------+--------------------------------+\n|                City|sum(CAST(Price_range AS DOUBLE))|\n+--------------------+--------------------------------+\n|           Bangalore|                            54.0|\n|           Tangerang|                             6.0|\n|               Kochi|                            41.0|\n|          Aurangabad|                            44.0|\n|           Faridabad|                           365.0|\n|            Armidale|                             2.0|\n|              Monroe|                             2.0|\n|            Savannah|                            47.0|\n|          New Delhi&quot;|                             3.0|\n|              Mysore|                            49.0|\n|Huda City Centre ...|                            null|\n|           Bras�_lia|                            62.0|\n|            Valdosta|                            45.0|\n|           Edinburgh|                            58.0|\n|           Singapore|                            69.0|\n|       San Juan City|                             6.0|\n|          Manchester|                            54.0|\n|             Jakarta|                            48.0|\n|          Beechworth|                             2.0|\n|               Patna|                            49.0|\n+--------------------+--------------------------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+--------------------+--------------------------------+\n","                City|sum(CAST(Price_range AS DOUBLE))|\n","+--------------------+--------------------------------+\n","           Bangalore|                            54.0|\n","           Tangerang|                             6.0|\n","               Kochi|                            41.0|\n","          Aurangabad|                            44.0|\n","           Faridabad|                           365.0|\n","            Armidale|                             2.0|\n","              Monroe|                             2.0|\n","            Savannah|                            47.0|\n","          New Delhi&quot;|                             3.0|\n","              Mysore|                            49.0|\n","Huda City Centre ...|                            null|\n","           Bras�_lia|                            62.0|\n","            Valdosta|                            45.0|\n","           Edinburgh|                            58.0|\n","           Singapore|                            69.0|\n","       San Juan City|                             6.0|\n","          Manchester|                            54.0|\n","             Jakarta|                            48.0|\n","          Beechworth|                             2.0|\n","               Patna|                            49.0|\n","+--------------------+--------------------------------+\n","only showing top 20 rows\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/zomato.csv\")  \ndf = df.withColumnRenamed(\"Price range\",\"Price_range\")\ndf = df.withColumnRenamed(\"Restaurant Name\",\"Restaurant_Name\")\ndf = df.withColumnRenamed(\"Average Cost for two\",\"Average_Cost_for_two\")\ndf.createOrReplaceTempView(\"test\")\n#df.agg({\"Average_Cost_for_two\": \"min\"}).show()\nlist1 = []\nfor i in df.collect():\n  if(i['Average_Cost_for_two']!=None):\n    if(i['Average_Cost_for_two'].isdigit()):\n      list1.append(int(i['Average_Cost_for_two']))\nmax_value = max(list1)      \n#print(max_value)\nstr1 = \"select * from test where Average_Cost_for_two =\"+\"'\"+str(max_value)+\"'\"\nprint(str1)\n#spark.sql(\"select * from test where Average_Cost_for_two = '800000' \").show()\ndf.select(\"Restaurant_Name\",\"City\").filter((df.Average_Cost_for_two == max_value)).show(100, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50f3b382-8c98-4011-82da-457d8ace401c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Restaurant ID","nullable":true,"type":"string"},{"metadata":{},"name":"Restaurant_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Country Code","nullable":true,"type":"string"},{"metadata":{},"name":"City","nullable":true,"type":"string"},{"metadata":{},"name":"Address","nullable":true,"type":"string"},{"metadata":{},"name":"Locality","nullable":true,"type":"string"},{"metadata":{},"name":"Locality Verbose","nullable":true,"type":"string"},{"metadata":{},"name":"Longitude","nullable":true,"type":"string"},{"metadata":{},"name":"Latitude","nullable":true,"type":"string"},{"metadata":{},"name":"Cuisines","nullable":true,"type":"string"},{"metadata":{},"name":"Average_Cost_for_two","nullable":true,"type":"string"},{"metadata":{},"name":"Currency","nullable":true,"type":"string"},{"metadata":{},"name":"Has Table booking","nullable":true,"type":"string"},{"metadata":{},"name":"Has Online delivery","nullable":true,"type":"string"},{"metadata":{},"name":"Is delivering now","nullable":true,"type":"string"},{"metadata":{},"name":"Switch to order menu","nullable":true,"type":"string"},{"metadata":{},"name":"Price_range","nullable":true,"type":"string"},{"metadata":{},"name":"Aggregate rating","nullable":true,"type":"string"},{"metadata":{},"name":"Rating color","nullable":true,"type":"string"},{"metadata":{},"name":"Rating text","nullable":true,"type":"string"},{"metadata":{},"name":"Votes","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">select * from test where Average_Cost_for_two =&apos;800000&apos;\n+------------------------+-------+\n|Restaurant_Name         |City   |\n+------------------------+-------+\n|Skye                    |Jakarta|\n|Satoo - Hotel Shangri-La|Jakarta|\n+------------------------+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">select * from test where Average_Cost_for_two =&apos;800000&apos;\n","+------------------------+-------+\n","Restaurant_Name         |City   |\n","+------------------------+-------+\n","Skye                    |Jakarta|\n","Satoo - Hotel Shangri-La|Jakarta|\n","+------------------------+-------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1 = sc.parallelize([(\"Rafferty\",31),(\"Jones\",33),(\"Heisenberg\", 33),(\"Robinson\", 34),(\"Smith\", 34),(\"Williams\", 39)])\nrdd2 = sc.parallelize([(31, \"Sales\"),(33, \"Engineering\"),(34, \"Clerical\"),(35, \"Marketing\")])\nemployees = rdd1.toDF([\"LastName\",\"DepartmentID\"])\ndepartments = rdd2.toDF([\"DepartmentID\",\"DepartmentName\"])\nemployees.show()\ndepartments.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2e4dfa4-23d0-4692-941d-d6c02d2b0104","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+------------+\n|  LastName|DepartmentID|\n+----------+------------+\n|  Rafferty|          31|\n|     Jones|          33|\n|Heisenberg|          33|\n|  Robinson|          34|\n|     Smith|          34|\n|  Williams|          39|\n+----------+------------+\n\n+------------+--------------+\n|DepartmentID|DepartmentName|\n+------------+--------------+\n|          31|         Sales|\n|          33|   Engineering|\n|          34|      Clerical|\n|          35|     Marketing|\n+------------+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+----------+------------+\n","  LastName|DepartmentID|\n","+----------+------------+\n","  Rafferty|          31|\n","     Jones|          33|\n","Heisenberg|          33|\n","  Robinson|          34|\n","     Smith|          34|\n","  Williams|          39|\n","+----------+------------+\n","\n","+------------+--------------+\n","DepartmentID|DepartmentName|\n","+------------+--------------+\n","          31|         Sales|\n","          33|   Engineering|\n","          34|      Clerical|\n","          35|     Marketing|\n","+------------+--------------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_inner_join = employees.join(departments,[\"DepartmentID\"])\nget_inner_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"71826446-5212-4d18-84e1-0119515a372d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----------+--------------+\n|DepartmentID|  LastName|DepartmentName|\n+------------+----------+--------------+\n|          31|  Rafferty|         Sales|\n|          33|     Jones|   Engineering|\n|          33|Heisenberg|   Engineering|\n|          34|  Robinson|      Clerical|\n|          34|     Smith|      Clerical|\n+------------+----------+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+------------+----------+--------------+\n","DepartmentID|  LastName|DepartmentName|\n","+------------+----------+--------------+\n","          31|  Rafferty|         Sales|\n","          33|     Jones|   Engineering|\n","          33|Heisenberg|   Engineering|\n","          34|  Robinson|      Clerical|\n","          34|     Smith|      Clerical|\n","+------------+----------+--------------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_left_join = employees.join(departments,[\"DepartmentID\"],\"left\")\nget_left_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b771a8f5-c8e8-407e-889e-b58004a89cc8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----------+--------------+\n|DepartmentID|  LastName|DepartmentName|\n+------------+----------+--------------+\n|          31|  Rafferty|         Sales|\n|          33|     Jones|   Engineering|\n|          33|Heisenberg|   Engineering|\n|          34|  Robinson|      Clerical|\n|          34|     Smith|      Clerical|\n|          39|  Williams|          null|\n+------------+----------+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+------------+----------+--------------+\n","DepartmentID|  LastName|DepartmentName|\n","+------------+----------+--------------+\n","          31|  Rafferty|         Sales|\n","          33|     Jones|   Engineering|\n","          33|Heisenberg|   Engineering|\n","          34|  Robinson|      Clerical|\n","          34|     Smith|      Clerical|\n","          39|  Williams|          null|\n","+------------+----------+--------------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_left_outer_join = employees.join(departments,[\"DepartmentID\"],\"left_outer\")\nget_left_outer_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8db4fdd-3c9c-4733-8adb-3d0d4ff2cf8b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----------+--------------+\n|DepartmentID|  LastName|DepartmentName|\n+------------+----------+--------------+\n|          31|  Rafferty|         Sales|\n|          33|     Jones|   Engineering|\n|          33|Heisenberg|   Engineering|\n|          34|  Robinson|      Clerical|\n|          34|     Smith|      Clerical|\n|          39|  Williams|          null|\n+------------+----------+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+------------+----------+--------------+\n","DepartmentID|  LastName|DepartmentName|\n","+------------+----------+--------------+\n","          31|  Rafferty|         Sales|\n","          33|     Jones|   Engineering|\n","          33|Heisenberg|   Engineering|\n","          34|  Robinson|      Clerical|\n","          34|     Smith|      Clerical|\n","          39|  Williams|          null|\n","+------------+----------+--------------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_right_join = employees.join(departments,[\"DepartmentID\"],\"right\")\nget_right_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce6c4d3b-de64-4276-83f7-8fc33192f57d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----------+--------------+\n|DepartmentID|  LastName|DepartmentName|\n+------------+----------+--------------+\n|          31|  Rafferty|         Sales|\n|          33|     Jones|   Engineering|\n|          33|Heisenberg|   Engineering|\n|          34|  Robinson|      Clerical|\n|          34|     Smith|      Clerical|\n|          35|      null|     Marketing|\n+------------+----------+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+------------+----------+--------------+\n","DepartmentID|  LastName|DepartmentName|\n","+------------+----------+--------------+\n","          31|  Rafferty|         Sales|\n","          33|     Jones|   Engineering|\n","          33|Heisenberg|   Engineering|\n","          34|  Robinson|      Clerical|\n","          34|     Smith|      Clerical|\n","          35|      null|     Marketing|\n","+------------+----------+--------------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_right_outer_join = employees.join(departments,[\"DepartmentID\"],\"rightouter\")\nget_right_outer_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ae9f2b3-42c0-4d82-aaaf-93515f595f0a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----------+--------------+\n|DepartmentID|  LastName|DepartmentName|\n+------------+----------+--------------+\n|          31|  Rafferty|         Sales|\n|          33|     Jones|   Engineering|\n|          33|Heisenberg|   Engineering|\n|          34|  Robinson|      Clerical|\n|          34|     Smith|      Clerical|\n|          35|      null|     Marketing|\n+------------+----------+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+------------+----------+--------------+\n","DepartmentID|  LastName|DepartmentName|\n","+------------+----------+--------------+\n","          31|  Rafferty|         Sales|\n","          33|     Jones|   Engineering|\n","          33|Heisenberg|   Engineering|\n","          34|  Robinson|      Clerical|\n","          34|     Smith|      Clerical|\n","          35|      null|     Marketing|\n","+------------+----------+--------------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_left_semi_join = employees.join(departments,[\"DepartmentID\"],\"leftsemi\")\nget_left_semi_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48c9e0c8-dfb8-4557-a711-159b47995f65","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----------+\n|DepartmentID|  LastName|\n+------------+----------+\n|          31|  Rafferty|\n|          33|     Jones|\n|          33|Heisenberg|\n|          34|  Robinson|\n|          34|     Smith|\n+------------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+------------+----------+\n","DepartmentID|  LastName|\n","+------------+----------+\n","          31|  Rafferty|\n","          33|     Jones|\n","          33|Heisenberg|\n","          34|  Robinson|\n","          34|     Smith|\n","+------------+----------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_right_semi_join = employees.join(departments,[\"DepartmentID\"],\"rightsemi\")\nget_right_semi_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ffcb3930-b08b-45d7-9a9c-5f1d9b6d4f09","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2738535655821203&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>get_right_semi_join <span class=\"ansi-blue-fg\">=</span> employees<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>departments<span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;DepartmentID&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;rightsemi&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> get_right_semi_join<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">join</span><span class=\"ansi-blue-fg\">(self, other, on, how)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1360</span>                 on <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jseq<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1361</span>             <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>how<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;how should be a string&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 1362</span><span class=\"ansi-red-fg\">             </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>other<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">,</span> on<span class=\"ansi-blue-fg\">,</span> how<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1363</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1364</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Unsupported join type &#39;rightsemi&#39;. Supported join types include: &#39;inner&#39;, &#39;outer&#39;, &#39;full&#39;, &#39;fullouter&#39;, &#39;full_outer&#39;, &#39;leftouter&#39;, &#39;left&#39;, &#39;left_outer&#39;, &#39;rightouter&#39;, &#39;right&#39;, &#39;right_outer&#39;, &#39;leftsemi&#39;, &#39;left_semi&#39;, &#39;semi&#39;, &#39;leftanti&#39;, &#39;left_anti&#39;, &#39;anti&#39;, &#39;cross&#39;.</div>","errorSummary":"<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Unsupported join type &#39;rightsemi&#39;. Supported join types include: &#39;inner&#39;, &#39;outer&#39;, &#39;full&#39;, &#39;fullouter&#39;, &#39;full_outer&#39;, &#39;leftouter&#39;, &#39;left&#39;, &#39;left_outer&#39;, &#39;rightouter&#39;, &#39;right&#39;, &#39;right_outer&#39;, &#39;leftsemi&#39;, &#39;left_semi&#39;, &#39;semi&#39;, &#39;leftanti&#39;, &#39;left_anti&#39;, &#39;anti&#39;, &#39;cross&#39;.","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n","<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n","<span class=\"ansi-green-fg\">&lt;command-2738535655821203&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n","<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>get_right_semi_join <span class=\"ansi-blue-fg\">=</span> employees<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>departments<span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;DepartmentID&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;rightsemi&#34;</span><span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> get_right_semi_join<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">join</span><span class=\"ansi-blue-fg\">(self, other, on, how)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1360</span>                 on <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jseq<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1361</span>             <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>how<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;how should be a string&#34;</span>\n","<span class=\"ansi-green-fg\">-&gt; 1362</span><span class=\"ansi-red-fg\">             </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>other<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">,</span> on<span class=\"ansi-blue-fg\">,</span> how<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1363</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1364</span> \n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n","<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n","</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n","<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n","\n","<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Unsupported join type &#39;rightsemi&#39;. Supported join types include: &#39;inner&#39;, &#39;outer&#39;, &#39;full&#39;, &#39;fullouter&#39;, &#39;full_outer&#39;, &#39;leftouter&#39;, &#39;left&#39;, &#39;left_outer&#39;, &#39;rightouter&#39;, &#39;right&#39;, &#39;right_outer&#39;, &#39;leftsemi&#39;, &#39;left_semi&#39;, &#39;semi&#39;, &#39;leftanti&#39;, &#39;left_anti&#39;, &#39;anti&#39;, &#39;cross&#39;.</div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_left_anti_join = employees.join(departments,[\"DepartmentID\"],\"leftanti\")\nget_left_anti_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91bb7058-1655-40e8-a1f1-ad3f56b3e676","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+--------+\n|DepartmentID|LastName|\n+------------+--------+\n|          39|Williams|\n+------------+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+------------+--------+\n","DepartmentID|LastName|\n","+------------+--------+\n","          39|Williams|\n","+------------+--------+\n","\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_right_anti_join = employees.join(departments,[\"DepartmentID\"],\"rightanti\")\nget_right_anti_join.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58f5944a-bf3e-4274-8ff9-5a8f80a8dbe7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2738535655821205&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>get_right_anti_join <span class=\"ansi-blue-fg\">=</span> employees<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>departments<span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;DepartmentID&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;rightanti&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> get_right_anti_join<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">join</span><span class=\"ansi-blue-fg\">(self, other, on, how)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1360</span>                 on <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jseq<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1361</span>             <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>how<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;how should be a string&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 1362</span><span class=\"ansi-red-fg\">             </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>other<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">,</span> on<span class=\"ansi-blue-fg\">,</span> how<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1363</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1364</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Unsupported join type &#39;rightanti&#39;. Supported join types include: &#39;inner&#39;, &#39;outer&#39;, &#39;full&#39;, &#39;fullouter&#39;, &#39;full_outer&#39;, &#39;leftouter&#39;, &#39;left&#39;, &#39;left_outer&#39;, &#39;rightouter&#39;, &#39;right&#39;, &#39;right_outer&#39;, &#39;leftsemi&#39;, &#39;left_semi&#39;, &#39;semi&#39;, &#39;leftanti&#39;, &#39;left_anti&#39;, &#39;anti&#39;, &#39;cross&#39;.</div>","errorSummary":"<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Unsupported join type &#39;rightanti&#39;. Supported join types include: &#39;inner&#39;, &#39;outer&#39;, &#39;full&#39;, &#39;fullouter&#39;, &#39;full_outer&#39;, &#39;leftouter&#39;, &#39;left&#39;, &#39;left_outer&#39;, &#39;rightouter&#39;, &#39;right&#39;, &#39;right_outer&#39;, &#39;leftsemi&#39;, &#39;left_semi&#39;, &#39;semi&#39;, &#39;leftanti&#39;, &#39;left_anti&#39;, &#39;anti&#39;, &#39;cross&#39;.","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n","<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n","<span class=\"ansi-green-fg\">&lt;command-2738535655821205&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n","<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>get_right_anti_join <span class=\"ansi-blue-fg\">=</span> employees<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>departments<span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;DepartmentID&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;rightanti&#34;</span><span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> get_right_anti_join<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">join</span><span class=\"ansi-blue-fg\">(self, other, on, how)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1360</span>                 on <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jseq<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1361</span>             <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>how<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;how should be a string&#34;</span>\n","<span class=\"ansi-green-fg\">-&gt; 1362</span><span class=\"ansi-red-fg\">             </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>other<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">,</span> on<span class=\"ansi-blue-fg\">,</span> how<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1363</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1364</span> \n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n","<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n","</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n","<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n","\n","<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Unsupported join type &#39;rightanti&#39;. Supported join types include: &#39;inner&#39;, &#39;outer&#39;, &#39;full&#39;, &#39;fullouter&#39;, &#39;full_outer&#39;, &#39;leftouter&#39;, &#39;left&#39;, &#39;left_outer&#39;, &#39;rightouter&#39;, &#39;right&#39;, &#39;right_outer&#39;, &#39;leftsemi&#39;, &#39;left_semi&#39;, &#39;semi&#39;, &#39;leftanti&#39;, &#39;left_anti&#39;, &#39;anti&#39;, &#39;cross&#39;.</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1 = sc.parallelize([(\"Rafferty\",31),(\"Jones\",33),(\"Heisenberg\", 33),(\"Robinson\", 34),(\"Smith\", 34),(\"Williams\", 39)])\nres2 = rdd1.toDF([\"Name\",\"Age\"])\nres2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0db4cc96-9b3c-4501-a605-8c5134db3835","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---+\n|      Name|Age|\n+----------+---+\n|  Rafferty| 31|\n|     Jones| 33|\n|Heisenberg| 33|\n|  Robinson| 34|\n|     Smith| 34|\n|  Williams| 39|\n+----------+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["rdd1 = sc.parallelize([(\"Rafferty\",31),(\"Jones\",33),(\"Heisenberg\", 33),(\"Robinson\", 34),(\"Smith\", 34),(\"Williams\", 39)])\nrdd2 = sc.parallelize([(31, \"Sales\"),(33, \"Engineering\"),(34, \"Clerical\"),(35, \"Marketing\")])\nemployees = rdd1.toDF([\"LastName\",\"DepartmentID\"])\ndepartments = rdd2.toDF([\"DepartmentID\",\"DepartmentName\"])\nemployees.show()\ndepartments.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9061a844-c1a9-4097-ab14-ffc0a484e7a8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------------+\n|  LastName|DepartmentID|\n+----------+------------+\n|  Rafferty|          31|\n|     Jones|          33|\n|Heisenberg|          33|\n|  Robinson|          34|\n|     Smith|          34|\n|  Williams|          39|\n+----------+------------+\n\n+------------+--------------+\n|DepartmentID|DepartmentName|\n+------------+--------------+\n|          31|         Sales|\n|          33|   Engineering|\n|          34|      Clerical|\n|          35|     Marketing|\n+------------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df1 =  employees.join(departments, \"DepartmentID\")  # select * from test where test.id = test1.id and test.name = test1.name\n#df1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80f6b01c-f742-4367-b1c3-4f2710899106","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------------+------------+--------------+\n|  LastName|DepartmentID|DepartmentID|DepartmentName|\n+----------+------------+------------+--------------+\n|  Rafferty|          31|          31|         Sales|\n|  Rafferty|          31|          33|   Engineering|\n|  Rafferty|          31|          34|      Clerical|\n|  Rafferty|          31|          35|     Marketing|\n|     Jones|          33|          31|         Sales|\n|     Jones|          33|          33|   Engineering|\n|     Jones|          33|          34|      Clerical|\n|     Jones|          33|          35|     Marketing|\n|Heisenberg|          33|          31|         Sales|\n|Heisenberg|          33|          33|   Engineering|\n|Heisenberg|          33|          34|      Clerical|\n|Heisenberg|          33|          35|     Marketing|\n|  Robinson|          34|          31|         Sales|\n|  Robinson|          34|          33|   Engineering|\n|  Robinson|          34|          34|      Clerical|\n|  Robinson|          34|          35|     Marketing|\n|     Smith|          34|          31|         Sales|\n|     Smith|          34|          33|   Engineering|\n|     Smith|          34|          34|      Clerical|\n|     Smith|          34|          35|     Marketing|\n+----------+------------+------------+--------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\nrdd1 = sc.parallelize([(\"Rafferty\",31),(\"Jones\",33),(\"Heisenberg\", 33),(\"Robinson\", 34),(\"Smith\", 34),(\"Williams\", 39)])\nemployees = rdd1.toDF([\"LastName\",\"DepartmentID\"])\nemployees.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"091e2c7d-a3f2-4fff-baae-6dc8631364a6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------------+\n|  LastName|DepartmentID|\n+----------+------------+\n|  Rafferty|          31|\n|     Jones|          33|\n|Heisenberg|          33|\n|  Robinson|          34|\n|     Smith|          34|\n|  Williams|          39|\n+----------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["employees = employees.withColumn(\"DepartmentID\", f.lit(7500))\nemployees.show()\nemployees = employees.withColumnRenamed(\"LastName\",\"LastName_name\")\nemployees.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c72001d-91b7-41f9-b9d4-1e2108683572","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------------+\n|  LastName|DepartmentID|\n+----------+------------+\n|  Rafferty|        7500|\n|     Jones|        7500|\n|Heisenberg|        7500|\n|  Robinson|        7500|\n|     Smith|        7500|\n|  Williams|        7500|\n+----------+------------+\n\n+-------------+------------+\n|LastName_name|DepartmentID|\n+-------------+------------+\n|     Rafferty|        7500|\n|        Jones|        7500|\n|   Heisenberg|        7500|\n|     Robinson|        7500|\n|        Smith|        7500|\n|     Williams|        7500|\n+-------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["display(df1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af83a568-ac51-4267-b32d-c0d1ec283a03","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Rafferty",31,31,"Sales"],["Rafferty",31,33,"Engineering"],["Rafferty",31,34,"Clerical"],["Rafferty",31,35,"Marketing"],["Jones",33,31,"Sales"],["Jones",33,33,"Engineering"],["Jones",33,34,"Clerical"],["Jones",33,35,"Marketing"],["Heisenberg",33,31,"Sales"],["Heisenberg",33,33,"Engineering"],["Heisenberg",33,34,"Clerical"],["Heisenberg",33,35,"Marketing"],["Robinson",34,31,"Sales"],["Robinson",34,33,"Engineering"],["Robinson",34,34,"Clerical"],["Robinson",34,35,"Marketing"],["Smith",34,31,"Sales"],["Smith",34,33,"Engineering"],["Smith",34,34,"Clerical"],["Smith",34,35,"Marketing"],["Williams",39,31,"Sales"],["Williams",39,33,"Engineering"],["Williams",39,34,"Clerical"],["Williams",39,35,"Marketing"]],"plotOptions":{"displayType":"barChart","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"LastName","type":"\"string\"","metadata":"{}"},{"name":"DepartmentID","type":"\"long\"","metadata":"{}"},{"name":"DepartmentID","type":"\"long\"","metadata":"{}"},{"name":"DepartmentName","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>LastName</th><th>DepartmentID</th><th>DepartmentID</th><th>DepartmentName</th></tr></thead><tbody><tr><td>Rafferty</td><td>31</td><td>31</td><td>Sales</td></tr><tr><td>Rafferty</td><td>31</td><td>33</td><td>Engineering</td></tr><tr><td>Rafferty</td><td>31</td><td>34</td><td>Clerical</td></tr><tr><td>Rafferty</td><td>31</td><td>35</td><td>Marketing</td></tr><tr><td>Jones</td><td>33</td><td>31</td><td>Sales</td></tr><tr><td>Jones</td><td>33</td><td>33</td><td>Engineering</td></tr><tr><td>Jones</td><td>33</td><td>34</td><td>Clerical</td></tr><tr><td>Jones</td><td>33</td><td>35</td><td>Marketing</td></tr><tr><td>Heisenberg</td><td>33</td><td>31</td><td>Sales</td></tr><tr><td>Heisenberg</td><td>33</td><td>33</td><td>Engineering</td></tr><tr><td>Heisenberg</td><td>33</td><td>34</td><td>Clerical</td></tr><tr><td>Heisenberg</td><td>33</td><td>35</td><td>Marketing</td></tr><tr><td>Robinson</td><td>34</td><td>31</td><td>Sales</td></tr><tr><td>Robinson</td><td>34</td><td>33</td><td>Engineering</td></tr><tr><td>Robinson</td><td>34</td><td>34</td><td>Clerical</td></tr><tr><td>Robinson</td><td>34</td><td>35</td><td>Marketing</td></tr><tr><td>Smith</td><td>34</td><td>31</td><td>Sales</td></tr><tr><td>Smith</td><td>34</td><td>33</td><td>Engineering</td></tr><tr><td>Smith</td><td>34</td><td>34</td><td>Clerical</td></tr><tr><td>Smith</td><td>34</td><td>35</td><td>Marketing</td></tr><tr><td>Williams</td><td>39</td><td>31</td><td>Sales</td></tr><tr><td>Williams</td><td>39</td><td>33</td><td>Engineering</td></tr><tr><td>Williams</td><td>39</td><td>34</td><td>Clerical</td></tr><tr><td>Williams</td><td>39</td><td>35</td><td>Marketing</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"449c31e4-ca0b-44c5-a5ae-ccb44d24be19","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a8444bf-80c3-4b71-a312-c9019098b67d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-4415491654619787>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/FileStore/tables/first_test.csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 610\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    611\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    461\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 462\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    463\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    464\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    818\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 819\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    820\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    821\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1048\u001B[0m             )\n\u001B[1;32m   1049\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1050\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1051\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1052\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   1865\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1866\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1867\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1868\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1869\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"storage_options\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"encoding\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"memory_map\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"compression\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m   1360\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHanldes\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1361\u001B[0m         \"\"\"\n\u001B[0;32m-> 1362\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m   1363\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1364\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    640\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"replace\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    641\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 642\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    643\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    644\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/FileStore/tables/first_test.csv'","errorSummary":"<span class='ansi-red-fg'>FileNotFoundError</span>: [Errno 2] No such file or directory: '/FileStore/tables/first_test.csv'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)\n","\u001B[0;32m<command-4415491654619787>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n","\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/FileStore/tables/first_test.csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n","\u001B[1;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    609\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 610\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    611\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n","\u001B[1;32m    460\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    461\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 462\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    463\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    464\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n","\u001B[1;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    818\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 819\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    820\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    821\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n","\u001B[1;32m   1048\u001B[0m             )\n","\u001B[1;32m   1049\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m-> 1050\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m   1051\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1052\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n","\u001B[1;32m   1865\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1866\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m-> 1867\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m   1868\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1869\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"storage_options\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"encoding\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"memory_map\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"compression\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n","\u001B[1;32m   1360\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHanldes\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1361\u001B[0m         \"\"\"\n","\u001B[0;32m-> 1362\u001B[0;31m         self.handles = get_handle(\n","\u001B[0m\u001B[1;32m   1363\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1364\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n","\u001B[1;32m    640\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"replace\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    641\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 642\u001B[0;31m             handle = open(\n","\u001B[0m\u001B[1;32m    643\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    644\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/FileStore/tables/first_test.csv'"]}}],"execution_count":0},{"cell_type":"code","source":["dfs = spark.read.option(\"inferSchema\",\"true\").option(\"multiLine\", \"true\").json(\"/FileStore/tables/test_multiLine.json\")\ndfs.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5699ccc-91d9-45a4-89ad-f8cfdeb90369","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+------+\n|age| id|  name|\n+---+---+------+\n| 21|  1|aakash|\n+---+---+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.write.format('csv').save('mycsv1')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3fb714e0-d565-4e77-a8f0-7b168af991db","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"321e0942-3fd3-498a-99c9-ed0d544b42b4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   area|  name|   zip|\n+---+-------+------+------+\n|  1|BLOCK A|   RAM|560097|\n|  2|BLOCK B|   RAJ|560091|\n|  3|BLOCK C| ROHAN|560092|\n|  4|BLOCK D|RAMESH|560092|\n|  6|BLOCK E|  RAMU|560098|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df1 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/UIDAI_ENR_DETAIL_20170308-034f9.csv\") \ndf1.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4119df26-dc60-4ec6-bd54-3bb1d083cbc5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[18]: 440818"]}],"execution_count":0},{"cell_type":"code","source":["df1.write.format('csv').save('mycsv2')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0ada4bf-cb10-4530-8abc-f27caca7ba29","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1.coalesce(1).write.format('csv').save('mycsv3')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"64283934-fabd-46a9-bbfc-d8ac411f8a07","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1.coalesce(20).write.format('csv').save('mycsv4')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9f37590c-217e-46e7-a097-516cef6a3ba3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["str1 = \"10\"\nres1 = int(str1)\nprint(res1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c6f08053-b721-489b-a94a-9dbfd5509677","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["10\n"]}],"execution_count":0},{"cell_type":"code","source":["data = [('James','','Smith','1991-04-01'),\n  ('Michael','Rose','','2000-05-19'),\n  ('Robert','','Williams','1978-09-05'),\n  ('Maria','Anne','Jones','1967-12-01'),\n  ('Jen','Mary','Brown','1980-02-17')\n]\nrdd1 = sc.parallelize(data)\n\ndf1 = rdd1.toDF([\"firstname\",\"middlename\",\"lastname\",\"dob\"])\ndf1.show()\n\ndf1 = df1.withColumn('year', f.split(df1['dob'], '-').getItem(0)) \\\n       .withColumn('month', f.split(df1['dob'], '-').getItem(1)) \\\n       .withColumn('day', f.split(df1['dob'], '-').getItem(2))\ndf1.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec947a41-cd13-46ab-996c-782ea05196f9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----------+--------+----------+\n|firstname|middlename|lastname|       dob|\n+---------+----------+--------+----------+\n|    James|          |   Smith|1991-04-01|\n|  Michael|      Rose|        |2000-05-19|\n|   Robert|          |Williams|1978-09-05|\n|    Maria|      Anne|   Jones|1967-12-01|\n|      Jen|      Mary|   Brown|1980-02-17|\n+---------+----------+--------+----------+\n\n+---------+----------+--------+----------+----+-----+---+\n|firstname|middlename|lastname|dob       |year|month|day|\n+---------+----------+--------+----------+----+-----+---+\n|James    |          |Smith   |1991-04-01|1991|04   |01 |\n|Michael  |Rose      |        |2000-05-19|2000|05   |19 |\n|Robert   |          |Williams|1978-09-05|1978|09   |05 |\n|Maria    |Anne      |Jones   |1967-12-01|1967|12   |01 |\n|Jen      |Mary      |Brown   |1980-02-17|1980|02   |17 |\n+---------+----------+--------+----------+----+-----+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data = [('James','','Smith','1991-04-01','M',3000),\n  ('Michael','Rose','','2000-05-19','M',4000),\n  ('Robert','','Williams','1978-09-05','M',4000),\n  ('Maria','Anne','Jones','1967-12-01','F',4000),\n  ('Jen','Mary','Brown','1980-02-17','F',-1)\n]\n\ncolumns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf2=df.select(f.concat(df.firstname,df.middlename,df.lastname)\n              .alias(\"FullName\"),\"dob\",\"gender\",\"salary\")\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6744fb52-062e-493d-85ab-68f019d7f2fb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------+----------+------+------+\n|FullName      |dob       |gender|salary|\n+--------------+----------+------+------+\n|JamesSmith    |1991-04-01|M     |3000  |\n|MichaelRose   |2000-05-19|M     |4000  |\n|RobertWilliams|1978-09-05|M     |4000  |\n|MariaAnneJones|1967-12-01|F     |4000  |\n|JenMaryBrown  |1980-02-17|F     |-1    |\n+--------------+----------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\ncolumns = [\"Name\",\"Dept\",\"Salary\"]\ndf = spark.createDataFrame(data,columns)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b383ede5-e34e-45ac-b2da-f50dbd9c0c7a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+---------+------+\n|   Name|     Dept|Salary|\n+-------+---------+------+\n|  James|    Sales|  3000|\n|Michael|    Sales|  4600|\n| Robert|    Sales|  4100|\n|  Maria|  Finance|  3000|\n|  James|    Sales|  3000|\n|  Scott|  Finance|  3300|\n|    Jen|  Finance|  3900|\n|   Jeff|Marketing|  3000|\n|  Kumar|Marketing|  2000|\n|   Saif|    Sales|  4100|\n+-------+---------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2=df.select(f.countDistinct(\"Salary\"))\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cce973a0-fdf6-4d70-a293-e461574001b2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------------+\n|count(DISTINCT Salary)|\n+----------------------+\n|                     6|\n+----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2=df.select(f.countDistinct(\"Dept\",\"Salary\"))\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"813a6556-0b20-4443-b71c-59a04605845b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------------------+\n|count(DISTINCT Dept, Salary)|\n+----------------------------+\n|                           8|\n+----------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2=df.select(f.collect_list(\"Salary\"))\ndf2 = df2.withColumnRenamed(\"collect_list(Salary)\",\"get_list\")\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de1bc4aa-f34b-432b-8ffb-f0cee0a0692a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------------------------------------------------+\n|get_list                                                    |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2=df.select(f.collect_set(\"Salary\"))\ndf2 = df2.withColumnRenamed(\"collect_set(Salary)\",\"get_set\")\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6998ecac-386f-4ee2-9ed5-b06b56330947","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------------------------+\n|get_set                             |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2=df.select(f.sum_distinct(\"Salary\"))\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a23b098b-0e6a-402a-a169-bdd522b8ec4b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|sum(DISTINCT Salary)|\n+--------------------+\n|               20900|\n+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data = [(1,\"20200828\"),(2,\"20180525\")]\ncolumns=[\"id\",\"date\"]\ndf=spark.createDataFrame(data,columns)\ndf = df.withColumn('year', f.substring('date', 1,4))\ndf = df.withColumn('month', f.substring('date', 5,2))\ndf = df.withColumn('day', f.substring('date', 7,2))\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f399d555-a4bb-45d4-9d0b-390e2de29082","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: long (nullable = true)\n |-- date: string (nullable = true)\n |-- year: string (nullable = true)\n |-- month: string (nullable = true)\n |-- day: string (nullable = true)\n\n+---+--------+----+-----+---+\n|id |date    |year|month|day|\n+---+--------+----+-----+---+\n|1  |20200828|2020|08   |28 |\n|2  |20180525|2018|05   |25 |\n+---+--------+----+-----+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data = [(1,\"20200828\"),(2,\"20180525\")]\ncolumns=[\"id\",\"date\"]\ndf=spark.createDataFrame(data,columns)\ndf.show()\ndf = df.withColumn('day', f.substring('date',7,2))\n#df.printSchema()\ndf.show(truncate=False)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7b372e5b-0c75-4b8e-981f-733b7ca9773c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------+\n| id|    date|\n+---+--------+\n|  1|20200828|\n|  2|20180525|\n+---+--------+\n\n+---+--------+---+\n|id |date    |day|\n+---+--------+---+\n|1  |20200828|28 |\n|2  |20180525|25 |\n+---+--------+---+\n\nroot\n |-- id: long (nullable = true)\n |-- date: string (nullable = true)\n |-- day: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame([([10, 20, 30,40,50,60],), ([40, 50,70,80,90],)], ['x'])\ndf.show(truncate=False)\ndf.select(f.slice(df.x, 1, 3).alias(\"sliced_data\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11519925-6916-4de5-821b-d169e9e97714","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------------+\n|x                       |\n+------------------------+\n|[10, 20, 30, 40, 50, 60]|\n|[40, 50, 70, 80, 90]    |\n+------------------------+\n\n+------------+\n|sliced_data |\n+------------+\n|[10, 20, 30]|\n|[40, 50, 70]|\n+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as f\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/test3.csv\")  \n#df1 = df1.limit(100)\ndf.select(\"Rating\").distinct().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bb20414d-cbad-43b6-bc1b-04ddc9140cfd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|              Rating|\n+--------------------+\n| that fills not o...|\n|25 Reviews , 24 F...|\n|     5/25/2019 15:54|\n|                   3|\n|8 Reviews , 25 Fo...|\n|                null|\n|                   5|\n|     5/22/2019 19:54|\n| including a PDR ...|\n| just one fried r...|\n|     5/17/2019 22:17|\n|     5/20/2019 12:11|\n|     5/19/2019 22:57|\n| rotis and raita ...|\n| across all my vi...|\n|9 Reviews , 8 Fol...|\n|                   1|\n|     5/24/2019 15:22|\n|                   4|\n|4 Reviews , 2 Fol...|\n+--------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# aakashkumar667@gmail.com\nfrom pyspark.sql.functions import udf,col\nfile_location1 = \"/FileStore/tables/first_test.csv\"\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(file_location1)\n#df.show()\n#df.printSchema()\n\ndict={}\nfor i in df.collect():\n    #print(i['id'])\n    #print(i['name'])\n    dict[i['id']]=i['name'] + \"_\" +  i['area']\nprint(dict)\n\n\n\ndef CheckValue(id):\n  #print(dict.get(id))\n  return str(dict.get(id)) + \"_new\"\n\na = udf(CheckValue) # here we are registering that function as udf , function will do for only primitive and data strutre data types , but if u have to pass the columns you must have to register the function as the udf\ndf_res = df.withColumn('res',a(col('id')))\ndf_res.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d38e0342-ba22-4059-8c7a-145a9a1cf3f6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["{1: 'RAM_BLOCK A', 2: 'RAJ_BLOCK B', 3: 'ROHAN_BLOCK C', 4: 'RAMESH_BLOCK D', 6: 'RAMU_BLOCK E'}\n+---+-------+------+------+------------------+\n| id|   area|  name|   zip|               res|\n+---+-------+------+------+------------------+\n|  1|BLOCK A|   RAM|560097|   RAM_BLOCK A_new|\n|  2|BLOCK B|   RAJ|560091|   RAJ_BLOCK B_new|\n|  3|BLOCK C| ROHAN|560092| ROHAN_BLOCK C_new|\n|  4|BLOCK D|RAMESH|560092|RAMESH_BLOCK D_new|\n|  6|BLOCK E|  RAMU|560098|  RAMU_BLOCK E_new|\n+---+-------+------+------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/UIDAI_ENR_DETAIL_20170308-034f9.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2614075-ae30-4860-8ef2-e905d5900183","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = df.filter(df[\"Aadhaar generated\"] == 0)\ndf2 = df.groupBy(\"State\").count()\ndf3 = df2.orderBy(f.desc(\"count\"))\ndf3.limit(10).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2df82f06-4dce-4e3a-87ea-13a55047f0a0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------+-----+\n|         State|count|\n+--------------+-----+\n|         Bihar| 2982|\n| Uttar Pradesh| 2854|\n|   West Bengal| 2770|\n|Madhya Pradesh| 1654|\n|     Rajasthan| 1143|\n|   Maharashtra| 1117|\n|       Gujarat| 1087|\n|     Karnataka| 1025|\n|        Odisha|  860|\n|    Tamil Nadu|  776|\n+--------------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2 = df.filter(df[\"Aadhaar generated\"] > 0)\ndf3 = df2.groupBy(\"Sub District\",\"Gender\").sum(\"Aadhaar generated\")\n\nget_male = df3.filter(df3[\"Gender\"] == \"M\")\nget_male = get_male.orderBy(f.desc(\"sum(Aadhaar generated)\")).limit(10)\nget_male.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10d0dd62-1fc6-429c-a891-02aaf8526499","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------+------+----------------------+\n|      Sub District|Gender|sum(Aadhaar generated)|\n+------------------+------+----------------------+\n|           Roorkee|     M|                  3274|\n|     Circus Avenue|     M|                  2515|\n|         Kahalgoan|     M|                  2381|\n|North 24 Paraganas|     M|                  2046|\n|         Ghaziabad|     M|                  1856|\n|    Ahmadabad City|     M|                  1823|\n|        Barddhaman|     M|                  1704|\n|          Varanasi|     M|                  1637|\n|            Chunar|     M|                  1601|\n|         Nathnagar|     M|                  1577|\n+------------------+------+----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["get_female = df3.filter(df3[\"Gender\"] == \"F\")\nget_female = get_female.orderBy(f.desc(\"sum(Aadhaar generated)\")).limit(10)\nget_female.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"77a797f7-d42a-41ec-80c8-17ba9b9cad20","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------+------+----------------------+\n|      Sub District|Gender|sum(Aadhaar generated)|\n+------------------+------+----------------------+\n|North 24 Paraganas|     F|                  2392|\n|        Barddhaman|     F|                  2326|\n|           Bisauli|     F|                  1902|\n|       Bhangar - I|     F|                  1445|\n|            Jaypur|     F|                  1358|\n|        Karandighi|     F|                  1269|\n|             Bausi|     F|                  1196|\n|     Circus Avenue|     F|                  1134|\n|          Raniganj|     F|                  1128|\n|            Chapra|     F|                  1095|\n+------------------+------+----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/tables/first_test.csv\")  \n#df.show(2)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3161bbc4-cb3c-40ad-bad8-5766bb63156a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: string (nullable = true)\n |-- area: string (nullable = true)\n |-- name: string (nullable = true)\n |-- zip: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/first_test.csv\")  \n#df.show(2)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"644a1728-2bfa-46aa-8ff9-a1fddaadfbc9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: integer (nullable = true)\n |-- area: string (nullable = true)\n |-- name: string (nullable = true)\n |-- zip: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df1 = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/first_test.csv\")\ndf1.show()\ndf1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d72ca00-1077-410e-a945-69077f7ad467","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   area|  name|   zip|\n+---+-------+------+------+\n|  1|BLOCK A|   RAM|560097|\n|  2|BLOCK B|   RAJ|560091|\n|  3|BLOCK C| ROHAN|560092|\n|  4|BLOCK D|RAMESH|560092|\n|  6|BLOCK E|  RAMU|560098|\n+---+-------+------+------+\n\nroot\n |-- id: string (nullable = true)\n |-- area: string (nullable = true)\n |-- name: string (nullable = true)\n |-- zip: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f3eedd7d-1288-41be-ab4d-740b5dc5ee8b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   area|  name|   zip|\n+---+-------+------+------+\n|  1|BLOCK A|   RAM|560097|\n|  2|BLOCK B|   RAJ|560091|\n|  3|BLOCK C| ROHAN|560092|\n|  4|BLOCK D|RAMESH|560092|\n|  6|BLOCK E|  RAMU|560098|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03217089-87d9-40ca-b2ef-2a18ed5a57f3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[26]: [Row(id=1, area='BLOCK A', name='RAM', zip=560097),\n Row(id=2, area='BLOCK B', name='RAJ', zip=560091),\n Row(id=3, area='BLOCK C', name='ROHAN', zip=560092),\n Row(id=4, area='BLOCK D', name='RAMESH', zip=560092),\n Row(id=6, area='BLOCK E', name='RAMU', zip=560098)]"]}],"execution_count":0},{"cell_type":"code","source":["dict1 = {}\nfor i in df.collect():\n  dict1[i[\"id\"]] = i[\"name\"] # here key is id and value is name in the dictinoary\nprint(dict1)   # from the dataframe u have created the dictinoary"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c773090-b433-44a8-a68b-27b8f502fd9b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["{1: 'RAM', 2: 'RAJ', 3: 'ROHAN', 4: 'RAMESH', 6: 'RAMU'}\n"]}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"/FileStore/tables/first_test.csv\")  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"202114fb-d85c-4356-861f-b047d470ef92","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03ca00dd-b9bd-4677-9235-2f33be19b24c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   area|  name|   zip|\n+---+-------+------+------+\n|  1|BLOCK A|   RAM|560097|\n|  2|BLOCK B|   RAJ|560091|\n|  3|BLOCK C| ROHAN|560092|\n|  4|BLOCK D|RAMESH|560092|\n|  6|BLOCK E|  RAMU|560098|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["res1 = df.filter(df[\"id\"]>3) # here we have Filtered the data where id is >3\nres1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6530cc62-056c-4638-a9da-0d2bf3307424","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   area|  name|   zip|\n+---+-------+------+------+\n|  4|BLOCK D|RAMESH|560092|\n|  6|BLOCK E|  RAMU|560098|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# convert as the JSON File\ndf.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6f84c02-6793-4187-94ed-4ec0e5a194be","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[4]: [Row(id=1, area='BLOCK A', name='RAM', zip=560097),\n Row(id=2, area='BLOCK B', name='RAJ', zip=560091),\n Row(id=3, area='BLOCK C', name='ROHAN', zip=560092),\n Row(id=4, area='BLOCK D', name='RAMESH', zip=560092),\n Row(id=6, area='BLOCK E', name='RAMU', zip=560098)]"]}],"execution_count":0},{"cell_type":"code","source":["dict1 = {}\nfor i in df.collect():\n  list1 = []\n  list1.append(i[\"area\"])\n  list1.append(i[\"name\"])\n  list1.append(i[\"zip\"])\n  dict1[i[\"id\"]] = list1\nprint(dict1)  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7b5191cf-ff7f-4938-a9c4-0f92377f1285","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m/databricks/spark/python/pyspark/sql/types.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   1559\u001B[0m             \u001B[0;31m# but this will not be used in normal cases\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1560\u001B[0;31m             \u001B[0midx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__fields__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1561\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: 'area' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-2735545042926293>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m   \u001B[0mlist1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m   \u001B[0mlist1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"area\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m   \u001B[0mlist1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m   \u001B[0mlist1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"zip\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/types.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   1563\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1564\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1565\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1566\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1567\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: area","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: area","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n","\u001B[0;32m/databricks/spark/python/pyspark/sql/types.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, item)\u001B[0m\n","\u001B[1;32m   1559\u001B[0m             \u001B[0;31m# but this will not be used in normal cases\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m-> 1560\u001B[0;31m             \u001B[0midx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__fields__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m   1561\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;31mValueError\u001B[0m: 'area' is not in list\n","\n","During handling of the above exception, another exception occurred:\n","\n","\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n","\u001B[0;32m<command-2735545042926293>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n","\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m      3\u001B[0m   \u001B[0mlist1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m----> 4\u001B[0;31m   \u001B[0mlist1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"area\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m      5\u001B[0m   \u001B[0mlist1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m      6\u001B[0m   \u001B[0mlist1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"zip\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/sql/types.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, item)\u001B[0m\n","\u001B[1;32m   1563\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1564\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m-> 1565\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m   1566\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1567\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;31mValueError\u001B[0m: area"]}}],"execution_count":0},{"cell_type":"code","source":["rdd1 = sc.parallelize([(\"Rafferty\",31),(\"Jones\",33),(\"Heisenberg\", 33),(\"Robinson\", 34),(\"Smith\", 34),(\"Williams\", 39)])\nrdd2 = sc.parallelize([(31, \"Sales\"),(33, \"Engineering\"),(34, \"Clerical\"),(35, \"Marketing\")])\nemployees = rdd1.toDF([\"LastName\",\"DepartmentID\"])\ndepartments = rdd2.toDF([\"DepartmentID\",\"DepartmentName\"])\n#employees.show()\n#departments.show()\n#df1 = employees.join(departments, \"DepartmentID\") # select * from test where test.id = test1.id and test.name = test1.name\n#df1.show()\n#inner_join.show()\n#inner_join_with_Multiple = employees.join(departments, [\"DepartmentID\"])\n#inner_join_with_Multiple.show()\n#left = employees.join(departments, \"DepartmentID\", \"left\")\n#left.show()\nleft_outer = employees.join(departments, \"DepartmentID\", \"left_outer\")\nleft_outer.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"507771ce-1c84-47a3-a2a1-e9f2f19cea0f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+----------+--------------+\n|DepartmentID|  LastName|DepartmentName|\n+------------+----------+--------------+\n|          31|  Rafferty|         Sales|\n|          33|     Jones|   Engineering|\n|          33|Heisenberg|   Engineering|\n|          34|  Robinson|      Clerical|\n|          34|     Smith|      Clerical|\n|          39|  Williams|          null|\n+------------+----------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9c04f31d-add3-4007-be14-e2a4e2506d05","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Pyspark_dataframe_Example","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
